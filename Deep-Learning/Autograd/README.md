# DerivaX — Automatic Differentiation 

DerivaX é um mini mecanismo de diferenciação automática (autograd) desenvolvido para demonstrar como o backpropagation funciona internamente em redes neurais.

O projeto implementa, do zero, um sistema que constrói grafos computacionais e calcula derivadas automaticamente utilizando a regra da cadeia.

## Objetivo do Projeto

O DerivaX foi criado com fins educacionais para:

- Demonstrar como redes neurais são apenas expressões matemáticas
- Mostrar como funciona o cálculo de gradientes
- Implementar backpropagation sem frameworks prontos
- Visualizar grafos computacionais
- Entender profundamente autograd (como PyTorch e TensorFlow fazem internamente)

## Créditos e Inspiração

Este projeto foi fortemente inspirado no micrograd, criado por Andrej Karpathy.
O micrograd é uma implementação educacional brilhante que demonstra como o backpropagation pode ser construído do zero de forma simples e elegante.
